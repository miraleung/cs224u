{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e74f5be-ed3b-42d2-8d48-92f94cdcf073",
   "metadata": {},
   "outputs": [],
   "source": [
    "from colors import ColorsCorpusReader\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch_color_describer import (\n",
    "    ContextualColorDescriber, create_example_dataset)\n",
    "import utils\n",
    "from utils import START_SYMBOL, END_SYMBOL, UNK_SYMBOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef6e0be1-0492-4e3c-a565-a4a027587c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.fix_random_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d81d71e9-0748-4102-aba3-05e93e00d0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS_SRC_FILENAME = os.path.join(\n",
    "    \"data\", \"colors\", \"filteredCorpus.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3c02089-f8bb-4b9a-a58d-e693c8502ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ColorsCorpusReader(\n",
    "    COLORS_SRC_FILENAME,\n",
    "    word_count=None,\n",
    "    normalize_colors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b29ccb6-af16-4b96-98e8-f21db834014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dev dataset\n",
    "dev_color_seqs, dev_word_seqs, dev_vocab = create_example_dataset(\n",
    "    group_size=100, vec_dim=2)\n",
    "dev_color_seqs_train, dev_color_seqs_test, dev_word_seqs_train, dev_word_seqs_test = \\\n",
    "    train_test_split(dev_color_seqs, dev_word_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae4a3aa1-b794-4e24-ad9e-66978ef86307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug dataset\n",
    "toy_color_seqs, toy_word_seqs, toy_vocab = create_example_dataset(\n",
    "    group_size=20, vec_dim=2)\n",
    "toy_color_seqs_train, toy_color_seqs_test, toy_word_seqs_train, toy_word_seqs_test = \\\n",
    "    train_test_split(toy_color_seqs, toy_word_seqs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba3b891-edd8-43d6-8441-eb7df067078b",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c8dc7a9-e619-455c-8b38-e9f323f9dbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch_color_describer import Encoder, Decoder\n",
    "\n",
    "class DeepEncoder(Encoder):\n",
    "    def __init__(self, *args, num_layers=2, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=self.color_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True)\n",
    "\n",
    "\n",
    "class DeepDecoder(Decoder):\n",
    "    def __init__(self, *args, num_layers=2, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=self.embed_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b9c4f80-fd69-4d31-9d4e-3aa55dfb0f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_color_describer import EncoderDecoder\n",
    "\n",
    "class DeepContextualColorDescriber(ContextualColorDescriber):\n",
    "    def __init__(self, *args, num_layers=2, **kwargs):\n",
    "        self.num_layers = num_layers\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def build_graph(self):\n",
    "        encoder = DeepEncoder(\n",
    "            color_dim=self.color_dim,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            num_layers=self.num_layers)  # The new piece is this argument.\n",
    "\n",
    "        decoder = DeepDecoder(\n",
    "            vocab_size=self.vocab_size,\n",
    "            embed_dim=self.embed_dim,\n",
    "            embedding=self.embedding,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            num_layers=self.num_layers)  # The new piece is this argument.\n",
    "\n",
    "        return EncoderDecoder(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb980032-1f19-4dec-a213-e31c29301719",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_baseline = DeepContextualColorDescriber(\n",
    "    dev_vocab,\n",
    "    embed_dim=10,\n",
    "    hidden_dim=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a69f63a-a28e-4295-b9f7-e26f75ccfcd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 611. Training loss did not improve more than tol=1e-05. Final error is 0.1754165217280388."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 11s, sys: 10.9 s, total: 1min 22s\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%time _ = mod_baseline.fit(dev_color_seqs_train, dev_word_seqs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7ca4f19-07d3-4f27-a747-768aaf4a73d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_baseline.listener_accuracy(dev_color_seqs_test, dev_word_seqs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82804003-7242-4b25-9d9f-73191bfe8ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listener acc: 1.0, BLEU: 1.0, Perp: [1.0023646685310066, 1.0045776168152944, 1.004586382066643, 1.0046199791826271, 1.002347660933698, 1.0045150748508154, 1.0046288850193703, 1.0044780435768654, 1.0023501277659816, 1.0023555428488387, 1.0046123460481107, 1.0023484230418926, 1.0044653661248146, 1.0023478013218756, 1.002350368433846, 1.0044664784173167, 1.002359995337962, 1.0046172739024042, 1.0045992762739389, 1.0046263301778775, 1.0023713074229, 1.0023518726132354, 1.0023548408870522, 1.0023747573092807, 1.004595501238899, 1.0023524943433537, 1.002364648474198, 1.0023486637081198, 1.0045050162987212, 1.0045666378131222, 1.004595008850345, 1.004657473616565, 1.0045957166594226, 1.0046176707321879, 1.0023507695474672, 1.0044715219030558, 1.0023597145480225, 1.0024246656298978, 1.00461394651042, 1.0023584710535005, 1.0023587919547219, 1.0023549211111569, 1.0047136972072794, 1.004700911270965, 1.004700295333324, 1.0044879941086238, 1.0046676441570044, 1.0045431042914095, 1.0023535372489514, 1.0023518124458866, 1.0045809860674473, 1.0023416844813724, 1.0023437501517956, 1.0044849159435467, 1.0044907218289731, 1.0046788623497833, 1.004482325445129, 1.002348743930247, 1.0046209743927155, 1.0046164190545857, 1.0044673773979487, 1.0045880130121565, 1.0046161830838019, 1.0046106943076474, 1.0023864511728442, 1.0023487238747126, 1.0044721466336677, 1.002362823311324, 1.0045903620165686, 1.0023745567331908, 1.0046255196151805, 1.0045937368543993, 1.0046321376343879, 1.00235913291272, 1.0046327327580469, 1.0023507695474672, 1.004714698700267, 1.0044950801629304, 1.0023663332517265, 1.004614818564703, 1.0045080338114718, 1.0046384891677103, 1.0046036156352587, 1.0045124077614247, 1.0023549612232188, 1.0045354678245957, 1.002352775125203, 1.0046605832007725, 1.0044809692563377, 1.004641423896789, 1.0023652902928732, 1.0046503310643866, 1.0045664575238646, 1.004514800519987, 1.0044626082754495, 1.0046488710019414, 1.00448474832205, 1.0044716133269265, 1.0046308960912504, 1.0044720704469037, 1.0046235496591014, 1.0044722228204606, 1.0044775864432967, 1.0023537578641533, 1.0024174435536428, 1.0023486637081198, 1.002358812011062, 1.004613538087362, 1.0023461367242625, 1.0046815310334356, 1.0045793456211267, 1.0044730913519415, 1.0046173629416015, 1.002362823311324, 1.0023531160750143, 1.0046025140258568, 1.004584279302611, 1.0024220977567175, 1.0044804054502083, 1.0045840536419601, 1.0046155162119448, 1.004469480114119, 1.0047509550451648, 1.0045016483203377, 1.0023606973141876, 1.004632507021196, 1.0023498269314761, 1.0046554108602506, 1.0045864641263518, 1.00463590337224, 1.004608037199834, 1.00236298376467, 1.0023574682398324, 1.0023519728921824, 1.0023773046395894, 1.002352273729264, 1.0045302550738258, 1.0046562421166916, 1.004592844412361, 1.0045272829519116, 1.0024683636303542, 1.0023656713734894, 1.0044913770969206, 1.0047048643594958, 1.0045759855687149, 1.0046963328643668, 1.0046236625204383, 1.0045531799941478, 1.0023637860329415, 1.0023557032975232, 1.004564566106068, 1.0047053768979128, 1.0047253147940087, 1.0046392785000384, 1.002370725760689, 1.0023602159588496, 1.0023639665436566, 1.0046685473276271, 1.0047552473710764, 1.0023635453521904, 1.0044647871256756, 1.0046476732226417, 1.002361319066201, 1.0023399998695557, 1.0023449935732698, 1.002346678218658, 1.0023527951810614, 1.0045735403004585, 1.0023632445015764, 1.0023493656526026, 1.002363926430153, 1.0023611786704503, 1.004502273144641, 1.0023775252757166, 1.002404785360968, 1.0046317066843133, 1.004673720141595, 1.0046201843800187, 1.0023520731711695, 1.002356585767124, 1.0023758003075363, 1.004587618018419, 1.0046232828962944, 1.002365330406595, 1.0044707905135886, 1.004583294604193, 1.004498173723078, 1.00236298376467, 1.0023496464309463, 1.004608088494886, 1.0044760169592937, 1.004599307049089, 1.0047092273411353, 1.004586154396621, 1.0046637715513587, 1.004493312435662, 1.004488786515188, 1.0024034212832806, 1.0023648490423573, 1.0023467784955264, 1.0023635252954715, 1.004723107321075, 1.0023563450932882, 1.002346377388294, 1.0046983654124984, 1.0024049859612543, 1.0045422811852212, 1.0023489845967821, 1.0045082471726747, 1.004547890568283, 1.0046451077850154, 1.004475956008706, 1.0046018819266997, 1.004639135623857, 1.004524646183123, 1.0023781671273728, 1.0046100787570575, 1.0023942540714479, 1.0024028796662583, 1.0044930838513753, 1.0046010509850516, 1.0045916442404388, 1.0023534770812028, 1.0046893320855579, 1.0046032258054736, 1.0045140994540172, 1.0044757274441667, 1.0046668982142668, 1.00446655460196, 1.0045914288260278, 1.004586751335702, 1.0023754192115164, 1.0044798873594847, 1.004513017378735, 1.002363886316656, 1.0023462971669244, 1.0044674078719389, 1.0046531531510032, 1.0023611987269812, 1.004615064792754, 1.0023515717766351, 1.0023531962985668, 1.004649027792207, 1.0045340960349642, 1.0044710343097818, 1.002346457609689, 1.0045966911848792, 1.002351611888161, 1.0024090983025078, 1.002349947265235, 1.0044873388517104, 1.0046294083057696, 1.0045353916138149, 1.002377204350505, 1.002352414120026, 1.004544156043113, 1.004639710253988, 1.0046538509846148, 1.0045847511396637, 1.0044965735998128, 1.0023630639913816, 1.004611258564674, 1.0023531762426763, 1.0024237829205243, 1.0044859369138648, 1.0045436225451838, 1.0023721899474534, 1.0044667374452219, 1.0045928475042376, 1.002352654790086, 1.0046425526547214, 1.004629572474445, 1.0046584182073452, 1.0044693429797849, 1.002348944485677, 1.0044906456351645, 1.004594321560813, 1.004599840486075, 1.0045172847517048, 1.0023489043745781, 1.004621271931092, 1.0046186146274358, 1.004607821760816, 1.004529310086706, 1.0023787086909866, 1.002362121329143, 1.0046524142720539, 1.0046606960912354, 1.0046194559306065, 1.0023555629049186, 1.0046124999379418, 1.0046067240527463, 1.0023503884895117, 1.002398827605674, 1.0024253477256342, 1.002356204700324, 1.0023596543787907, 1.0023492854302765, 1.0044923371445171, 1.0045082319325813, 1.0023784880538176, 1.0046709900221016, 1.0045233201910868, 1.0044666307866323, 1.0046660020431601, 1.0023532364103527, 1.00236649370732, 1.0023501678772764, 1.0023553222320654, 1.004571991357307, 1.0023482024313874, 1.0023476809891472, 1.0044895941635559, 1.0023531561867873, 1.0023459361710796, 1.0046431888676497, 1.0023650897243606, 1.0023519728921824, 1.004517421918768, 1.0044675754789678, 1.004576514728708, 1.0023727515556056, 1.0045532257246617, 1.0044975336722426, 1.0046396897314436, 1.004465411835344, 1.0023471996588116, 1.0047107461000089, 1.0023421056360962, 1.0045808328877075, 1.0047424867481234, 1.0046182965750827, 1.0046632002297904, 1.0045803305071714, 1.0046108789733392, 1.0044789121335118, 1.0023615798013754, 1.0046135053555658, 1.0023637659762032, 1.0024129699830204, 1.0044675297679462, 1.0049868833001099, 1.004469282031222, 1.0023522135618188, 1.0045671435279986, 1.0045933506397884, 1.0023570470592884, 1.0046187069653478, 1.0023436097658878, 1.0023595139839725, 1.0023840642164028, 1.002356485486331, 1.0045039190325944, 1.0045989172310072, 1.002350589046258, 1.0023515116093584, 1.0046276024627352, 1.0046414136353938, 1.0044942267754107, 1.002352574566707, 1.0023416644264032, 1.0046245551541382, 1.004498478509907, 1.0046177220306831, 1.0046046517669407, 1.0044687334949878, 1.0044962535766893, 1.0045957679500712, 1.004473502762901, 1.0046260736701997, 1.0045899722227827, 1.0023719292012392, 1.0045892188669687, 1.002355442568463, 1.0046032976161439, 1.0023493054858557, 1.00459369582246, 1.0045220246900786, 1.0045954089159166, 1.0023526949017854, 1.0045238079112782, 1.0023510703831042, 1.0023628634246509, 1.0023674163289764, 1.0046426757924873, 1.0047203030276914, 1.0023700036991334, 1.0023616600276375, 1.0044752703158681, 1.0044698610433163, 1.0023474002130057, 1.0023624422350388, 1.0046286800042326, 1.005271761476048, 1.0023407418995556, 1.0023566459356188, 1.0046211927707913, 1.0045944754313179, 1.004876678747484, 1.0044738532247532, 1.0048875499024916, 1.0023619007065778, 1.0045850261954288, 1.0046366729493559, 1.0044705619549261, 1.0024086369145184, 1.004627058662182, 1.0045004291565611, 1.004466996473461, 1.004592895701983, 1.004587869406147, 1.0045892439267041, 1.0045542622857664, 1.004634620752884, 1.0046218789618355, 1.002347440323864, 1.0047078407515668, 1.0044756360184237, 1.004654312788135, 1.002341082832995, 1.0045180315512938, 1.004470059126784, 1.0045079423710255, 1.0046260531496054, 1.0023620812159348, 1.0023462570562491, 1.0044736246626063, 1.0046481863156698, 1.0046133514646565, 1.002414073322616, 1.0023630840480635, 1.0023555027366837, 1.0023528152369214, 1.0045707565217576, 1.0023580899838338, 1.002347701044598, 1.0046583356727639, 1.0023526949017854, 1.0045849460293341, 1.0023674363860067, 1.0023552018957254, 1.0046684344309884, 1.0023595139839725, 1.004600168755975, 1.0045159283268983, 1.0023493455970187, 1.0045191441354218, 1.0044976860651138, 1.0046514906787165, 1.0045752537988517, 1.0045291424281653, 1.0045786891773318, 1.0044890150945853, 1.0046558407222062, 1.0044719942601685, 1.0045373731035114, 1.0045404520724315, 1.0046085706692705, 1.0023478013218756, 1.0049570811805635, 1.004609114399775, 1.0046451898783093, 1.0045905774293782, 1.0023634049551922, 1.0046715853068873, 1.004497046015832, 1.0044649394937082, 1.0044838949784174, 1.0046022717528345, 1.004475620780804, 1.0045071041688745, 1.0023484631529145, 1.0023656713734894, 1.004476199811168, 1.0045847060313795, 1.0045885361495492, 1.0045906389759545, 1.0050117854353495, 1.0044928400284225, 1.0023567061041283, 1.0046202869788246, 1.0047068654711049, 1.004469312505501, 1.002361539688254, 1.0044717961747924, 1.0023747974245178, 1.0046088887000701, 1.0023515717766351, 1.0045180010696235, 1.0045749351970645, 1.0023655109184224, 1.0045507105618603, 1.004626240141916, 1.0045876745125069, 1.0045908133580643, 1.0047425753226666, 1.0023531561867873, 1.002361258896584, 1.0046337690999643, 1.0024917606411337, 1.0044872017051851, 1.002350950048806, 1.0023653504634582, 1.004470074363982, 1.0044781045280866, 1.004477693107703, 1.0044701505499891, 1.0044699981780039, 1.0023887579170583, 1.0044789578471236, 1.004474721763283, 1.0047552781796885, 1.0045983325058696, 1.0045889259394347, 1.0046050108242193, 1.0052979076309543, 1.0044676211899999, 1.0023443718617615, 1.004993727260515, 1.0044704248198535, 1.0045763916476955, 1.0044684135162096, 1.0023560041390833, 1.002346999104778, 1.0044939067560255, 1.0023572877337983, 1.0023940534797522, 1.0044819140161012, 1.0046677262631898, 1.0044828587803079, 1.0023531762426763, 1.0046302188904295, 1.0046130334239671, 1.0045877360578384, 1.004579909754912, 1.0046149409158864, 1.0046170654113287, 1.002349686542164, 1.004549704505556, 1.0046256529986008, 1.0023726713257923, 1.0023744765027998, 1.0023687802106909, 1.0045938804662804, 1.0044978689367117, 1.0044651528091482, 1.0023484832084277, 1.0023453144572325, 1.0044713238181455, 1.0045714577854594, 1.0023580699275516, 1.0045080033313183, 1.004471476191136, 1.0046041080533432, 1.0044711714452705, 1.0023443116962134, 1.0051656053867772, 1.0023541589831997, 1.0046926271309937, 1.0023754192115164, 1.0045953371091934, 1.0047208382796295, 1.0045553293395484, 1.004619469679167, 1.004474295112308, 1.0044786683277582, 1.0023538180319693, 1.0045744326224604, 1.0045432109906065, 1.0048144252287137, 1.0023546603829108, 1.0045109142068316, 1.0044659908362839, 1.0044686420724278, 1.004560131151701, 1.0044679716415919, 1.0045990711064394, 1.0046268329342556, 1.0023879555688362, 1.0023430081128877, 1.004498798536575, 1.004641680431903, 1.0045881463607227, 1.0023489244301267, 1.00449735080095, 1.0044689772886848, 1.002363886316656, 1.0023520130037724, 1.0023409223936717, 1.0046003534081236, 1.0045624123829169, 1.0044672859760058, 1.0044665850758254, 1.0044666155496955, 1.0023443718617615, 1.0023642473383605, 1.0044796435525476, 1.004612099824724, 1.0045870844050702, 1.0046600348717016, 1.004584279302611, 1.0045129259360206, 1.002429961951432, 1.0045936650485128, 1.004730418134288, 1.0023523338967237, 1.004581950912044, 1.0023562648687276, 1.0023646083605853, 1.004553774491639, 1.0024227798454648, 1.004669552102909, 1.0045395527647498, 1.0045522196557626, 1.004467225028067, 1.0045110056486304, 1.0046277650523918, 1.0044676211899999, 1.00460115357018, 1.0023788892124512, 1.0047217212507702, 1.0023728919578403, 1.0045247376311721, 1.0045813559989274, 1.0045117371845196, 1.0023472397696376, 1.004598927489364, 1.0023517522785519, 1.0044743712999156, 1.002355041447362, 1.0045979324321481, 1.0046113728712558, 1.0046043029693017, 1.0045490338041518, 1.004478058814669, 1.002355783521904, 1.0023570270030897, 1.0045969476400467, 1.004719626769927, 1.0023799522859365, 1.0023447128001395, 1.0046266892893965, 1.0046120485282384, 1.002349425819364, 1.0045877052851693, 1.0023516921112319, 1.0044846873688134, 1.0046072575173015, 1.0046160907475137, 1.0023728919578403, 1.0023462971669244, 1.0045003834380632, 1.0046456226153568, 1.0046418651374691, 1.0044671031322447, 1.0044865769277544, 1.0045202872077237, 1.0046153831138647, 1.004469769620243, 1.004971775471471, 1.0023822589703286, 1.0023546002148924, 1.0023580498712712, 1.0044639643402957, 1.0046231392549885, 1.0023452542914582, 1.0046485557437859, 1.004475392216646, 1.0023523740083717, 1.0045035380388132, 1.0023469389385993, 1.0050801370253977, 1.0023455751757548, 1.0024358602594234, 1.0023597346044364, 1.0045864440709222, 1.0045920545544966, 1.0046591655317045, 1.0023526347342389, 1.0046150237547162, 1.0023523740083717, 1.004470683852847, 1.0046152153834826, 1.002354479878899, 1.0045920853280983, 1.0048674875836152, 1.0046146646723857, 1.0023467584401495, 1.004619599568226, 1.002368679925018, 1.0046002713404727, 1.0046250681645241, 1.004624770618277, 1.004668465220972, 1.0045914903729694, 1.0044677887972069, 1.0023563250371457, 1.0045744915414117, 1.002382720309755, 1.0024744434325379, 1.0045996455761772, 1.0046201228207705, 1.0045860640857145, 1.0044812892551147, 1.004695039439633, 1.0023501879329262, 1.0046229463747844, 1.0023749378278988, 1.0023654306909275, 1.0023568665535576, 1.0023643676790444, 1.002353998535504, 1.0045962398248978, 1.002371126906897, 1.0045845993101432, 1.0045893670187396, 1.0023566258794523, 1.0023628634246509, 1.0023586114477343, 1.0046212514111843, 1.0023606973141876, 1.0024054272824492, 1.004641105793884, 1.0045212473934835, 1.00451166098274, 1.0023490447634522, 1.002350248099885, 1.0046379670011838, 1.0024285174890037, 1.0045888986962384, 1.0023952169138222, 1.0023894599738596, 1.0023568464973733, 1.0045214150454358, 1.0047233948039993, 1.0046314809490764, 1.002350488767865, 1.0023574682398324, 1.0024120070724343, 1.0023821586792614, 1.0045915929512639, 1.0023640066571664, 1.004616008670863, 1.0023610783878192, 1.004785422440278, 1.0046728579929867, 1.0023583707719532, 1.00463018810865, 1.0045055192143657, 1.0023442114203323]\n"
     ]
    }
   ],
   "source": [
    "baseline_eval = mod_baseline.evaluate(dev_color_seqs_test, dev_word_seqs_test)\n",
    "baseline_perp = mod_baseline.perplexities(dev_color_seqs_test, dev_word_seqs_test)\n",
    "print(\"Listener acc: {0}, BLEU: {1}, Perp: {2}\".format(\n",
    "    baseline_eval['listener_accuracy'], baseline_eval['corpus_bleu'], baseline_perp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08bb68ef-ea91-4bca-abbe-5753ece8b0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/miniconda3/envs/nlu/lib/python3.8/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "Finished epoch 1 of 1000; error is 3.3078125715255737"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['batch_size', 'max_iter', 'eta', 'optimizer_class', 'l2_strength', 'gradient_accumulation_steps', 'max_grad_norm', 'validation_fraction', 'early_stopping', 'n_iter_no_change', 'warm_start', 'tol', 'hidden_dim', 'embed_dim', 'embedding', 'freeze_embedding']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 361. Training loss did not improve more than tol=1e-05. Final error is 0.30913403630256653..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'embed_dim': 5, 'eta': 0.001, 'hidden_dim': 10}\n",
      "Best score: 1.000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "print(mod_baseline.params)\n",
    "\n",
    "['batch_size', 'max_iter', 'eta', 'optimizer_class', 'l2_strength', \n",
    "  'gradient_accumulation_steps', 'max_grad_norm', 'validation_fraction', \n",
    "  'early_stopping', 'n_iter_no_change', 'warm_start', 'tol', 'hidden_dim',\n",
    "  'embed_dim', 'embedding', 'freeze_embedding']\n",
    "\"\"\"\n",
    "best_baseline_mod = utils.fit_classifier_with_hyperparameter_search(\n",
    "    dev_color_seqs_train,\n",
    "    dev_word_seqs_train,\n",
    "    mod_baseline,\n",
    "    cv=2,\n",
    "    scoring=None,\n",
    "    param_grid={'hidden_dim': [10, 20], 'embed_dim' : [5, 10], 'eta' : [0.001, 0.01, 0.05]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd04633c-2f80-459c-8e0f-3fd479ac3113",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
