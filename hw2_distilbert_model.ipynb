{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "867469eb-ba30-4e16-87d6-bbaf9b63d15b",
   "metadata": {},
   "source": [
    "# HW2 DistilBert Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "964663d5-e47e-49c8-b82e-e890d6ce52b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertConfig, BertModel, BertTokenizer\n",
    "from transformers import DistilBertConfig, DistilBertModel, DistilBertTokenizer\n",
    "from transformers import RobertaConfig, RobertaModel, RobertaTokenizer\n",
    "from transformers import XLNetConfig, XLNetModel, XLNetTokenizer\n",
    "\n",
    "from torch_shallow_neural_classifier import TorchShallowNeuralClassifier\n",
    "from torch_rnn_classifier import TorchRNNModel\n",
    "from torch_rnn_classifier import TorchRNNClassifier\n",
    "from torch_rnn_classifier import TorchRNNClassifierModel\n",
    "from torch_rnn_classifier import TorchRNNClassifier\n",
    "\n",
    "import sst\n",
    "import utils\n",
    "\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc251375-a16f-4006-aa38-9338900b783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SST_PATH = os.path.join('data', 'sentiment')\n",
    "DYNASENT_PATH = os.path.join('data', 'dynasent/dynasent-v1.1')\n",
    "TWITTER_PATH = os.path.join('data', 'twitter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c56f6703-381c-4fd5-9430-546e23267d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dynasent_dataset(*src_filenames, labels=None):\n",
    "    data = []\n",
    "    for filename in src_filenames:\n",
    "        with open(filename) as f:\n",
    "            for line in f:\n",
    "                d = json.loads(line)\n",
    "                if labels is None or d['gold_label'] in labels:\n",
    "                    data.append(d)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d692683a-046b-4e49-8acc-730c71fb88bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.fix_random_seeds()\n",
    "\n",
    "bakeoff_dev = sst.bakeoff_dev_reader(SST_PATH)\n",
    "# SST train label count: \n",
    "# Positive: 3610\n",
    "# Negative: 3310\n",
    "# Neutral:  1624\n",
    "sst_train = sst.train_reader(SST_PATH, include_subtrees=True, dedup=True)\n",
    "sst_dev = sst.dev_reader(SST_PATH)\n",
    "\n",
    "# Dynasent Yelp train label count: \n",
    "# Positive: 9,577 \n",
    "# Negative: 10,222 \n",
    "# Neutral:  5,201\n",
    "dynasent_train = load_dynasent_dataset(os.path.join(DYNASENT_PATH, 'dynasent-v1.1-round01-yelp-train.jsonl'))\n",
    "dynasent_dev = load_dynasent_dataset(os.path.join(DYNASENT_PATH, 'dynasent-v1.1-round01-yelp-dev.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bdde84f-538b-4a1e-bcd1-8e93442a4e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Dynasent data\n",
    "def extract_data_to_df(label_name, count):\n",
    "  dynasent_train_temp = load_dynasent_dataset(\n",
    "      os.path.join(DYNASENT_PATH, 'dynasent-v1.1-round01-yelp-train.jsonl'), \n",
    "      labels=(label_name, label_name))[:count]\n",
    "  pairs_temp = zip((d['text_id'], d['sentence'], d['gold_label'], 0) for d in dynasent_train_temp)\n",
    "  df_src = [p[0] for p in list(pairs_temp)]\n",
    "  return pd.DataFrame(df_src, columns=['example_id', 'sentence', 'label', 'is_subtree'])\n",
    "\n",
    "dynasent_positive_df = extract_data_to_df('positive', 500) #2000)\n",
    "dynasent_negative_df = extract_data_to_df('negative', 500) #2000)\n",
    "dynasent_neutral_df = extract_data_to_df('neutral', 2000)\n",
    "dynasent_df = pd.concat([dynasent_positive_df, dynasent_negative_df, dynasent_neutral_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ea9c90a-833a-427d-8f50-cd7ac5dc8260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process datasets\n",
    "# TODO: Add cell above for processing examples from the dynasent dataset. \n",
    "train_df = sst_train\n",
    "augmented_train_df = pd.concat([sst_train, dynasent_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b13054a-2124-4f8c-b6c3-5c994fbb06f4",
   "metadata": {},
   "source": [
    "# DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb3eb31d-dba2-4e70-86c4-62fa99c1d791",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HfDistilBertModel(nn.Module):\n",
    "    def __init__(self, n_classes, weights_name='distilbert-base-cased', dropout_p=0.5):\n",
    "        super().__init__()\n",
    "        self.weights_name = weights_name\n",
    "        self.dropout_p = dropout_p\n",
    "        self.distilbert = DistilBertModel.from_pretrained(self.weights_name)\n",
    "        self.distilbert.train()\n",
    "        self.hidden_dim = self.distilbert.embeddings.word_embeddings.embedding_dim\n",
    "        self.n_classes = n_classes\n",
    "        self.preclassifier_layer = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.classifier_layer = nn.Linear(self.hidden_dim, self.n_classes)\n",
    "\n",
    "    def forward(self, indices, mask):\n",
    "        reps = self.distilbert(indices, attention_mask=mask)\n",
    "        hidden_state = reps[0]\n",
    "        pooler = hidden_state[:,0]\n",
    "        pooler = self.preclassifier_layer(pooler)\n",
    "        pooler = nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        return self.classifier_layer(pooler)\n",
    "        \"\"\"\n",
    "        return self.classifier_layer(reps.pooler_output)\n",
    "        \"\"\"\n",
    "\n",
    "class HfDistilBertClassifier(TorchShallowNeuralClassifier):\n",
    "    def __init__(self, weights_name, *args, **kwargs):\n",
    "        self.weights_name = weights_name\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(self.weights_name)\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.params += ['weights_name']\n",
    "\n",
    "    def build_graph(self):\n",
    "        return HfDistilBertModel(self.n_classes_, self.weights_name)\n",
    "\n",
    "    def build_dataset(self, X, y=None):\n",
    "        data = self.tokenizer.batch_encode_plus(\n",
    "            X,\n",
    "            max_length=None,\n",
    "            add_special_tokens=True,\n",
    "            padding='longest',\n",
    "            return_attention_mask=True)\n",
    "        indices = torch.tensor(data['input_ids'])\n",
    "        mask = torch.tensor(data['attention_mask'])\n",
    "        if y is None:\n",
    "            dataset = torch.utils.data.TensorDataset(indices, mask)\n",
    "            return dataset\n",
    "        self.classes_ = sorted(set(y))\n",
    "        self.n_classes_ = len(self.classes_)\n",
    "        class_to_ind = dict(zip(self.classes_, range(self.n_classes_)))\n",
    "        return torch.utils.data.TensorDataset(\n",
    "            indices, \n",
    "            mask,\n",
    "            torch.tensor([class_to_ind[y0] for y0 in y]))\n",
    "\n",
    "def distilbert_fine_tune_phi(text):\n",
    "    return text\n",
    "\n",
    "# Uses the best values from parameter tuning.\n",
    "def fit_distilbert_classifier(X, y):\n",
    "    mod = HfDistilBertClassifier(\n",
    "        gradient_accumulation_steps=8,\n",
    "        #eta=0.0005,\n",
    "        eta=0.0001,\n",
    "        #hidden_dim=325,\n",
    "        hidden_dim=300,\n",
    "        weights_name='distilbert-base-cased',\n",
    "        batch_size=8,\n",
    "        max_iter=1,  \n",
    "        n_iter_no_change=5,\n",
    "        early_stopping=True)  \n",
    "\n",
    "    mod.fit(X, y)\n",
    "\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da2bf8a-2a4b-471b-967a-c184ded3ebd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Finished epoch 1 of 1; error is 1375.383385562338"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assessment dataset 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.696     0.897     0.784       428\n",
      "     neutral      0.514     0.162     0.246       229\n",
      "    positive      0.780     0.838     0.808       444\n",
      "\n",
      "    accuracy                          0.720      1101\n",
      "   macro avg      0.663     0.632     0.612      1101\n",
      "weighted avg      0.692     0.720     0.682      1101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# took around an hour maybe to run\n",
    "# without gpu: still has not completed after 5h\n",
    "# Start: 2021/09/08 03:15\n",
    "distilbert_classifier_xval = sst.experiment(\n",
    "    train_df,\n",
    "    distilbert_fine_tune_phi,\n",
    "    fit_distilbert_classifier,\n",
    "    assess_dataframes=[sst_dev, bakeoff_dev],\n",
    "    vectorize=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cceab54-469b-4faa-9883-c06847ae9bb7",
   "metadata": {},
   "source": [
    "## Train for real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d9f2a5-dfa1-4ee5-8069-0bb85ef19000",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_distlbert_classifier = distilbert_classifier_xval['model']\n",
    "\n",
    "# Remove the rest of the experiment results to clear out some memory\n",
    "#del distilbert_classifier_xval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cabd7e-d13a-46cd-8b0f-28f4259ab71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_distilbert_classifier_bakeoff(X, y):\n",
    "    mod = HfDistilBertClassifier(\n",
    "        gradient_accumulation_steps=8,\n",
    "        eta=0.0001,\n",
    "        hidden_dim=300,\n",
    "        weights_name='distilbert-base-cased',\n",
    "        batch_size=8,\n",
    "        max_iter=10, #100,  \n",
    "        n_iter_no_change=5,\n",
    "        early_stopping=True)  \n",
    "    mod.fit(X, y)\n",
    "    return mood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a964a425-16b0-48d2-a4d6-4e06153c3c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_optimized_distilbert_classifier(X, y):\n",
    "    optimized_distilbert_classifier.max_iter = 10 #100 #1000\n",
    "    optimized_distilbert_classifier.fit(X, y)\n",
    "    return optimized_distilbert_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242963d0-a09f-4853-b582-f37da895d3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start time: 2021/09/08 \n",
    "hfdistilbert_experiment = sst.experiment(\n",
    "    augmented_train_df, \n",
    "    distilbert_fine_tune_phi,\n",
    "    fit_distilbert_classifier,\n",
    "    assess_dataframes=[sst_dev, bakeoff_dev],\n",
    "    vectorize=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0f0082-ef17-42c3-b36b-c4fc803624b6",
   "metadata": {},
   "source": [
    "## Bakeoff submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7d8a01-bc8d-4059-aadd-590f60262803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one_rnn_distilbert(text):\n",
    "    # List of tokenized examples:\n",
    "    X = [hfdistilbert_experiment['phi'](text)]\n",
    "    # Standard `predict` step on a list of lists of str:\n",
    "    preds = hfdistilbert_experiment['model'].predict(X)\n",
    "    # Be sure to return the only member of the predictions,\n",
    "    # rather than the singleton list:\n",
    "    return preds[0]\n",
    "\n",
    "def create_bakeoff_submission_distilbert(\n",
    "        predict_one_func,\n",
    "        output_filename='cs224u-sentiment-bakeoff-entry-v1distilbert.csv'):\n",
    "\n",
    "    bakeoff_test = sst.bakeoff_test_reader(SST_PATH)\n",
    "    sst_test = sst.test_reader(SST_PATH)\n",
    "    bakeoff_test['dataset'] = 'bakeoff'\n",
    "    sst_test['dataset'] = 'sst3'\n",
    "    df = pd.concat((bakeoff_test, sst_test))\n",
    "\n",
    "    df['prediction'] = df['sentence'].apply(predict_one_func)\n",
    "\n",
    "    df.to_csv(output_filename, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a262a77-71c1-45e8-bf7f-425e44c67fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bakeoff_submission_distilbert(predict_one_rnn_distilbert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b58fd4-31df-48fe-9396-07c08f8e99f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56c778b-e5cd-47f2-959a-af8cad44ca80",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bakeoff_submission_bert(predict_one_rnn_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b54e22-923b-42c3-8412-0fe464532187",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
